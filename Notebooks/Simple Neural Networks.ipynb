{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Networks\n",
    "** November 2017 **\n",
    "\n",
    "** Andrew Riberio @ [AndrewRib.com](http://www.andrewrib.com) **\n",
    "\n",
    "Simple Neural Networks \n",
    "\n",
    "** Note: ** This notebook contains interactive elements and certain latex snippets that will not render in github markdown. \n",
    "You must run this notebook on your local Jupyter notebook environment for interactive elements or render or if you wish to render just the latex by using the url of this repo with the [online NBViewer](https://nbviewer.jupyter.org/).\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#Interactive Components\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\overset{∧}{\\rightarrow}\n",
    "\\begin{bmatrix} \n",
    "0  \\\\\n",
    "0 \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^\\intercal \n",
    "relu(X^\\intercal W + c)+b = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}^\\intercal\n",
    "$$\n",
    "\n",
    "Here we simply implement the results in section 6.1 of the book. We are not really learning XOR here. We are using the results of some learning process to show that given some matrix X, we can transform it into a vector which represents the XOR operation on each row of X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result we've achieved by hand is: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 1\n",
    "\\end{bmatrix}^\\intercal \n",
    "relu\\left(\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "-1  \\\\\n",
    "\\end{bmatrix}\n",
    "\\right)^\\intercal +0 = 1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matrix([[1,1],[1,1]])\n",
    "X = np.matrix([[1,1]])\n",
    "c = np.matrix([0,-1])\n",
    "w = np.matrix([1,-2])\n",
    "b = 0\n",
    "\n",
    "#w*relu(X*W.T+c)\n",
    "\n",
    "#relu(X*W+c)*w.T\n",
    "\n",
    "w*relu(X*W+c).T+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*W +c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.matmul(np.matrix([3,4,5]),np.matrix([3,4,5]).T).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*3 + 4*4 + 5*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, 3],name='input')\n",
    "W1 = tf.Variable(tf.random_uniform([3,2], -1, 1),name='W1')\n",
    "W2 = tf.Variable(tf.random_uniform([1,2], -1, 1),name='W2')\n",
    "\n",
    "t0 = tf.matmul(X, W1)\n",
    "t1 = tf.transpose(t0)\n",
    "\n",
    "y = tf.nn.relu(tf.matmul(W2,t1),name='relu')\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_xs = np.matrix([[1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
    "batch_ys = np.matrix([0,1,1,0]).T\n",
    "\n",
    "sess.run(train_step, feed_dict={X: batch_xs, y_: batch_ys}) \n",
    "\n",
    "sess.run(W1*tf.transpose(W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1. ,  1. ],\n",
       "        [ 0.5,  0.5],\n",
       "        [ 0.5,  0.5],\n",
       "        [ 1. ,  1. ]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([1,1/2,1/2,1]).T*np.matrix([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-a479eaefcadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 't'"
     ]
    }
   ],
   "source": [
    "tf.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def linLayer(x,w,b):\n",
    "    return x.T*w + b\n",
    "\n",
    "# x: 2 by 4\n",
    "# W: 2 by 2\n",
    "# c: 2 by 1\n",
    "# w: 2 by 1\n",
    "# b: scalar\n",
    "\n",
    "def fullNetwork(x,W,c,w,b):\n",
    "    h = relu( linLayer(W,x,c) )\n",
    "    return linLayer(w, h, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matrix([[1,0],[1,0],[1,0]])\n",
    "X = np.matrix([0,1,1])\n",
    "w = np.matrix([-1,-1]).T\n",
    "\n",
    "\n",
    "#w.T*relu(X*W).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(np.matrix([3,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"x.T\\n\")\n",
    "print(x.T)\n",
    "print(\"\\nx.T*W\\n\")\n",
    "print(x.T*W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.T*W + c\n",
      "\n",
      "[[ 0 -1]\n",
      " [ 1  0]\n",
      " [ 1  0]\n",
      " [ 2  1]]\n"
     ]
    }
   ],
   "source": [
    "linLayH = linLayer(W,x,c)\n",
    "print(\"x.T*W + c\\n\")\n",
    "print(linLayH.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu( x.T*W + c )\n",
      "\n",
      "[[0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 1]]\n"
     ]
    }
   ],
   "source": [
    "h = relu( linLayH )\n",
    "print(\"relu( x.T*W + c )\\n\")\n",
    "print(h.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.T*relu( x.T*W + c )+b\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "w = np.matrix([1,-2]).T\n",
    "b = 0\n",
    "\n",
    "f = linLayer(w, h, b)\n",
    "\n",
    "print(\"w.T*relu( x.T*W + c )+b\\n\")\n",
    "print(f.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullNetwork(np.matrix([0,0]).T,W,c,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If learning XOR was a linear transformation there would be an M such that x.T * M = [ 0,1,1,0]. There is no such M, however. Below you will find an interactive section for exploring M space ( 1 by 2 matrix ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\n",
      "[[ 1]\n",
      " [-2]]\n",
      "\n",
      "x:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "Desired x.T * M:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "x.T * M:\n",
      "[[ 0]\n",
      " [-2]\n",
      " [ 1]\n",
      " [-1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.exploreM>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exploreM(m1=1,m2=-2):\n",
    "    M = np.matrix([m1,m2]).T\n",
    "    print(\"M:\")\n",
    "    print(M)\n",
    "    print(\"\\nx:\")\n",
    "    print(x.T)\n",
    "    print(\"\\nDesired x.T * M:\")\n",
    "    print(np.matrix([0,1,1,0]).T)\n",
    "    print(\"\\nx.T * M:\")\n",
    "    print(x.T*M)\n",
    "\n",
    "interact(exploreM,m1=[-10,10],m2=[-10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing AND\n",
    "We'd like to transform matrix X representing all possible values for the opperands of AND( ∧ ) into a vector representing the value of applying AND to those opperands:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\overset{∧}{\\rightarrow}\n",
    "\\begin{bmatrix} \n",
    "0  \\\\\n",
    "0 \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^\\intercal \n",
    "relu(X^\\intercal W + c)+b = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}^\\intercal\n",
    "$$\n",
    "\n",
    "As you can see, the task is to find matricies W,w,c, and scalar b which transforms our x matrix into the solution desired. Let's try to do this by hand to explore how the various variables behave in respect to transforming X.\n",
    "\n",
    "We first look at how a matrix of ones is transformed by different values of W. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.wInt>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wInt(w1,w2,w3,w4):\n",
    "    W = np.matrix([[w1,w2],\n",
    "                   [w3, w4]])\n",
    "    print(x.T*W)\n",
    "    \n",
    "interact(wInt,w1=[-5,5],w2=[-5,5],w3=[-5,5],w4=[-5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experimenting above we can see that: \n",
    "\n",
    "* w1: Modifies the first column values. \n",
    "* w3: Modifies the first column values. \n",
    "\n",
    "\n",
    "* w2: Modifies the second column values\n",
    "* w4: Modifies the second column values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same experiment with the x matrix set to the desired x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.wInt>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.matrix([[0,0,1,1],[0,1,0,1]])\n",
    "interact(wInt,w1=[-5,5],w2=[-5,5],w3=[-5,5],w4=[-5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experimenting above we can see that: \n",
    "\n",
    "* w1: Modifies the first column values in **the last two rows**. \n",
    "* w2: Modifies the second column values in **the last two rows**.\n",
    "\n",
    "\n",
    "* w3: Modifies the first column values in **the second and forth rows**.\n",
    "* w4: Modifies the second column values in **the second and forth rows**.\n",
    "\n",
    "By experimenting with the applet above, we can find a matrix that makes the last column distinctly larger than all others. Since we'd like to transform this matrix to [0,0,0,1], this is a property we need. I found a W matrix of the form [0,1],[0,1] that seems to do the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "W = np.matrix([[0,1],[0,1]])\n",
    "print(x.T*W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c\n",
    "C acts as a sort of filter on W. We can use this to get rid of the 1's above and keep the last row a 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.T*W + c\n",
      "\n",
      "[[ 0 -1]\n",
      " [ 0  0]\n",
      " [ 0  0]\n",
      " [ 0  1]]\n"
     ]
    }
   ],
   "source": [
    "c = np.matrix([0,-1]).T\n",
    "linLayH = linLayer(W,x,c)\n",
    "print(\"x.T*W + c\\n\")\n",
    "print(linLayH.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We can see that when applying an element wise relu will get us what we'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "h = relu(linLayH)\n",
    "print(h.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must figure out the other variable values to flatten this matrix to a vector. \n",
    "\n",
    "### w\n",
    "Given the results from above, we know we can ignore b and just find a w that turns the matrix above to a vector. We do that with a column vector [0,1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "w = np.matrix([0,1]).T\n",
    "fn = linLayer(w, h, 0)\n",
    "print(fn.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result we've achieved by hand is: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\overset{∧}{\\rightarrow}\n",
    "\\begin{bmatrix} \n",
    "0  \\\\\n",
    "0 \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}^\\intercal \n",
    "relu(\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "-1  \\\\\n",
    "\\end{bmatrix}\n",
    ")^\\intercal +0 = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result we've achieved by hand is: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\overset{∧}{\\rightarrow}\n",
    "\\begin{bmatrix} \n",
    "0  \\\\\n",
    "0 \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}^\\intercal \n",
    "relu(\\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "-1  \\\\\n",
    "\\end{bmatrix}\n",
    ")^\\intercal +0 = \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our next notebook **Neural Boolean Connectives 2** we will explore the case when we are given an additional target vector which contains a label for each row of X representing the result of the boolean operation to be learned. In the case of AND, we'd have: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x = \\begin{bmatrix} \n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    ",y = \n",
    "\\begin{bmatrix} \n",
    "0  \\\\\n",
    "0 \\\\\n",
    "0  \\\\\n",
    "1  \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
